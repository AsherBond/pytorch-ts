{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 10:54:21.219616: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 10:54:21.978515: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 10:54:21.978586: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 10:54:21.978593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "\n",
    "from pts.model.deepar import DeepAREstimator\n",
    "from pts.dataset.repository.datasets import dataset_recipes\n",
    "from pts.modules import StudentTOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepeare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\"solar_nips\", regenerate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaData(freq='H', target=None, feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='137')], feat_static_real=[], feat_dynamic_real=[], feat_dynamic_cat=[], prediction_length=24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouper = MultivariateGrouper(max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))\n",
    "\n",
    "test_grouper = MultivariateGrouper(num_test_dates=int(len(dataset.test)/len(dataset.train)), \n",
    "                                   max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_grouper(dataset.train)\n",
    "dataset_test = test_grouper(dataset.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultivariateEvaluator(quantiles=(np.arange(20)/20.0)[1:],\n",
    "                                  target_agg_funcs={'sum': np.sum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DeepVAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepAREstimator(\n",
    "    distr_output=StudentTOutput(dim=int(dataset.metadata.feat_static_cat[0].cardinality)),\n",
    "    input_size=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "    hidden_size=64,\n",
    "    num_layers=3,\n",
    "    dropout_rate=0.1,\n",
    "    lags_seq=[1, 24*7],\n",
    "  \n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    context_length=dataset.metadata.prediction_length,\n",
    "    freq=dataset.metadata.freq,\n",
    "    scaling=\"std\",\n",
    "    trainer_kwargs=dict(max_epochs=50, accelerator='gpu', devices='1'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kashif/.env/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | In sizes | Out sizes        \n",
      "---------------------------------------------------------------------\n",
      "0 | model | DeepARModel | 252 K  | ?        | [1, 100, 24, 137]\n",
      "---------------------------------------------------------------------\n",
      "252 K     Trainable params\n",
      "0         Non-trainable params\n",
      "252 K     Total params\n",
      "1.009     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e36c0c16134387a99dc5b0852e7f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 683.26031 (best 683.26031), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'train_loss' reached 567.28973 (best 567.28973), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
      "Epoch 2, global step 150: 'train_loss' reached 532.46063 (best 532.46063), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
      "Epoch 3, global step 200: 'train_loss' reached 517.28900 (best 517.28900), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
      "Epoch 4, global step 250: 'train_loss' reached 507.24280 (best 507.24280), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
      "Epoch 5, global step 300: 'train_loss' reached 499.54697 (best 499.54697), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
      "Epoch 6, global step 350: 'train_loss' reached 494.94312 (best 494.94312), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=6-step=350.ckpt' as top 1\n",
      "Epoch 7, global step 400: 'train_loss' reached 492.32855 (best 492.32855), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=7-step=400.ckpt' as top 1\n",
      "Epoch 8, global step 450: 'train_loss' reached 484.80246 (best 484.80246), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=8-step=450.ckpt' as top 1\n",
      "Epoch 9, global step 500: 'train_loss' reached 482.96368 (best 482.96368), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=9-step=500.ckpt' as top 1\n",
      "Epoch 10, global step 550: 'train_loss' reached 476.91800 (best 476.91800), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
      "Epoch 11, global step 600: 'train_loss' reached 472.81555 (best 472.81555), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=11-step=600.ckpt' as top 1\n",
      "Epoch 12, global step 650: 'train_loss' reached 470.03836 (best 470.03836), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=12-step=650.ckpt' as top 1\n",
      "Epoch 13, global step 700: 'train_loss' reached 468.26883 (best 468.26883), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=13-step=700.ckpt' as top 1\n",
      "Epoch 14, global step 750: 'train_loss' reached 465.82587 (best 465.82587), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=14-step=750.ckpt' as top 1\n",
      "Epoch 15, global step 800: 'train_loss' reached 463.00027 (best 463.00027), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=15-step=800.ckpt' as top 1\n",
      "Epoch 16, global step 850: 'train_loss' reached 461.00784 (best 461.00784), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
      "Epoch 17, global step 900: 'train_loss' reached 459.64880 (best 459.64880), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
      "Epoch 18, global step 950: 'train_loss' reached 458.66159 (best 458.66159), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=18-step=950.ckpt' as top 1\n",
      "Epoch 19, global step 1000: 'train_loss' reached 453.62851 (best 453.62851), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=19-step=1000.ckpt' as top 1\n",
      "Epoch 20, global step 1050: 'train_loss' was not in top 1\n",
      "Epoch 21, global step 1100: 'train_loss' reached 450.98367 (best 450.98367), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=21-step=1100.ckpt' as top 1\n",
      "Epoch 22, global step 1150: 'train_loss' reached 449.99997 (best 449.99997), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=22-step=1150.ckpt' as top 1\n",
      "Epoch 23, global step 1200: 'train_loss' reached 448.23047 (best 448.23047), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=23-step=1200.ckpt' as top 1\n",
      "Epoch 24, global step 1250: 'train_loss' reached 447.92499 (best 447.92499), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=24-step=1250.ckpt' as top 1\n",
      "Epoch 25, global step 1300: 'train_loss' reached 445.21585 (best 445.21585), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=25-step=1300.ckpt' as top 1\n",
      "Epoch 26, global step 1350: 'train_loss' reached 444.48956 (best 444.48956), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=26-step=1350.ckpt' as top 1\n",
      "Epoch 27, global step 1400: 'train_loss' reached 443.07202 (best 443.07202), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
      "Epoch 28, global step 1450: 'train_loss' reached 439.79269 (best 439.79269), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=28-step=1450.ckpt' as top 1\n",
      "Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
      "Epoch 30, global step 1550: 'train_loss' was not in top 1\n",
      "Epoch 31, global step 1600: 'train_loss' reached 439.06357 (best 439.06357), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=31-step=1600.ckpt' as top 1\n",
      "Epoch 32, global step 1650: 'train_loss' reached 437.61478 (best 437.61478), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=32-step=1650.ckpt' as top 1\n",
      "Epoch 33, global step 1700: 'train_loss' reached 437.38101 (best 437.38101), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=33-step=1700.ckpt' as top 1\n",
      "Epoch 34, global step 1750: 'train_loss' reached 436.70605 (best 436.70605), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=34-step=1750.ckpt' as top 1\n",
      "Epoch 35, global step 1800: 'train_loss' reached 435.17978 (best 435.17978), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=35-step=1800.ckpt' as top 1\n",
      "Epoch 36, global step 1850: 'train_loss' reached 434.79910 (best 434.79910), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=36-step=1850.ckpt' as top 1\n",
      "Epoch 37, global step 1900: 'train_loss' was not in top 1\n",
      "Epoch 38, global step 1950: 'train_loss' reached 432.09274 (best 432.09274), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=38-step=1950.ckpt' as top 1\n",
      "Epoch 39, global step 2000: 'train_loss' reached 431.26270 (best 431.26270), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=39-step=2000.ckpt' as top 1\n",
      "Epoch 40, global step 2050: 'train_loss' was not in top 1\n",
      "Epoch 41, global step 2100: 'train_loss' was not in top 1\n",
      "Epoch 42, global step 2150: 'train_loss' reached 428.73181 (best 428.73181), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=42-step=2150.ckpt' as top 1\n",
      "Epoch 43, global step 2200: 'train_loss' was not in top 1\n",
      "Epoch 44, global step 2250: 'train_loss' was not in top 1\n",
      "Epoch 45, global step 2300: 'train_loss' was not in top 1\n",
      "Epoch 46, global step 2350: 'train_loss' was not in top 1\n",
      "Epoch 47, global step 2400: 'train_loss' reached 427.44592 (best 427.44592), saving model to '/home/kashif/pytorch-ts/examples/lightning_logs/version_37/checkpoints/epoch=47-step=2400.ckpt' as top 1\n",
      "Epoch 48, global step 2450: 'train_loss' was not in top 1\n",
      "Epoch 49, global step 2500: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "predictor = estimator.train(dataset_train, cache_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 7it [00:00, 88.03it/s]\n",
      "Running evaluation: 7it [00:00, 98.35it/s]\n",
      "Running evaluation: 7it [00:00, 99.19it/s]\n",
      "Running evaluation: 7it [00:00, 97.87it/s]\n",
      "Running evaluation: 7it [00:00, 98.91it/s]\n",
      "Running evaluation: 7it [00:00, 99.60it/s]\n",
      "Running evaluation: 7it [00:00, 92.95it/s]\n",
      "Running evaluation: 7it [00:00, 99.10it/s]\n",
      "Running evaluation: 7it [00:00, 99.15it/s]\n",
      "Running evaluation: 7it [00:00, 98.41it/s]\n",
      "Running evaluation: 7it [00:00, 97.88it/s]\n",
      "Running evaluation: 7it [00:00, 98.10it/s]\n",
      "Running evaluation: 7it [00:00, 94.85it/s]\n",
      "Running evaluation: 7it [00:00, 101.79it/s]\n",
      "Running evaluation: 7it [00:00, 101.44it/s]\n",
      "Running evaluation: 7it [00:00, 101.46it/s]\n",
      "Running evaluation: 7it [00:00, 100.72it/s]\n",
      "Running evaluation: 7it [00:00, 100.74it/s]\n",
      "Running evaluation: 7it [00:00, 101.57it/s]\n",
      "Running evaluation: 7it [00:00, 101.28it/s]\n",
      "Running evaluation: 7it [00:00, 101.20it/s]\n",
      "Running evaluation: 7it [00:00, 100.77it/s]\n",
      "Running evaluation: 7it [00:00, 101.12it/s]\n",
      "Running evaluation: 7it [00:00, 99.43it/s]\n",
      "Running evaluation: 7it [00:00, 100.74it/s]\n",
      "Running evaluation: 7it [00:00, 101.17it/s]\n",
      "Running evaluation: 7it [00:00, 100.64it/s]\n",
      "Running evaluation: 7it [00:00, 101.55it/s]\n",
      "Running evaluation: 7it [00:00, 84.37it/s]\n",
      "Running evaluation: 7it [00:00, 97.93it/s]\n",
      "Running evaluation: 7it [00:00, 99.68it/s]\n",
      "Running evaluation: 7it [00:00, 95.76it/s]\n",
      "Running evaluation: 7it [00:00, 98.67it/s]\n",
      "Running evaluation: 7it [00:00, 99.83it/s]\n",
      "Running evaluation: 7it [00:00, 100.80it/s]\n",
      "Running evaluation: 7it [00:00, 99.78it/s]\n",
      "Running evaluation: 7it [00:00, 100.91it/s]\n",
      "Running evaluation: 7it [00:00, 102.41it/s]\n",
      "Running evaluation: 7it [00:00, 99.68it/s]\n",
      "Running evaluation: 7it [00:00, 100.27it/s]\n",
      "Running evaluation: 7it [00:00, 100.44it/s]\n",
      "Running evaluation: 7it [00:00, 100.30it/s]\n",
      "Running evaluation: 7it [00:00, 99.87it/s]\n",
      "Running evaluation: 7it [00:00, 100.48it/s]\n",
      "Running evaluation: 7it [00:00, 101.18it/s]\n",
      "Running evaluation: 7it [00:00, 100.70it/s]\n",
      "Running evaluation: 7it [00:00, 100.26it/s]\n",
      "Running evaluation: 7it [00:00, 100.71it/s]\n",
      "Running evaluation: 7it [00:00, 99.25it/s]\n",
      "Running evaluation: 7it [00:00, 99.51it/s]\n",
      "Running evaluation: 7it [00:00, 99.64it/s]\n",
      "Running evaluation: 7it [00:00, 99.47it/s]\n",
      "Running evaluation: 7it [00:00, 99.81it/s]\n",
      "Running evaluation: 7it [00:00, 100.73it/s]\n",
      "Running evaluation: 7it [00:00, 99.35it/s]\n",
      "Running evaluation: 7it [00:00, 99.68it/s]\n",
      "Running evaluation: 7it [00:00, 99.49it/s]\n",
      "Running evaluation: 7it [00:00, 101.03it/s]\n",
      "Running evaluation: 7it [00:00, 98.00it/s]\n",
      "Running evaluation: 7it [00:00, 100.65it/s]\n",
      "Running evaluation: 7it [00:00, 98.15it/s]\n",
      "Running evaluation: 7it [00:00, 101.33it/s]\n",
      "Running evaluation: 7it [00:00, 101.46it/s]\n",
      "Running evaluation: 7it [00:00, 99.93it/s]\n",
      "Running evaluation: 7it [00:00, 94.33it/s]\n",
      "Running evaluation: 7it [00:00, 101.39it/s]\n",
      "Running evaluation: 7it [00:00, 98.85it/s]\n",
      "Running evaluation: 7it [00:00, 96.74it/s]\n",
      "Running evaluation: 7it [00:00, 96.86it/s]\n",
      "Running evaluation: 7it [00:00, 96.17it/s]\n",
      "Running evaluation: 7it [00:00, 98.42it/s]\n",
      "Running evaluation: 7it [00:00, 101.37it/s]\n",
      "Running evaluation: 7it [00:00, 102.23it/s]\n",
      "Running evaluation: 7it [00:00, 101.34it/s]\n",
      "Running evaluation: 7it [00:00, 102.03it/s]\n",
      "Running evaluation: 7it [00:00, 102.05it/s]\n",
      "Running evaluation: 7it [00:00, 101.80it/s]\n",
      "Running evaluation: 7it [00:00, 102.23it/s]\n",
      "Running evaluation: 7it [00:00, 101.43it/s]\n",
      "Running evaluation: 7it [00:00, 101.80it/s]\n",
      "Running evaluation: 7it [00:00, 102.13it/s]\n",
      "Running evaluation: 7it [00:00, 102.11it/s]\n",
      "Running evaluation: 7it [00:00, 102.15it/s]\n",
      "Running evaluation: 7it [00:00, 102.35it/s]\n",
      "Running evaluation: 7it [00:00, 102.03it/s]\n",
      "Running evaluation: 7it [00:00, 103.04it/s]\n",
      "Running evaluation: 7it [00:00, 102.66it/s]\n",
      "Running evaluation: 7it [00:00, 101.71it/s]\n",
      "Running evaluation: 7it [00:00, 102.66it/s]\n",
      "Running evaluation: 7it [00:00, 101.86it/s]\n",
      "Running evaluation: 7it [00:00, 100.78it/s]\n",
      "Running evaluation: 7it [00:00, 102.58it/s]\n",
      "Running evaluation: 7it [00:00, 101.96it/s]\n",
      "Running evaluation: 7it [00:00, 102.21it/s]\n",
      "Running evaluation: 7it [00:00, 102.23it/s]\n",
      "Running evaluation: 7it [00:00, 102.68it/s]\n",
      "Running evaluation: 7it [00:00, 102.56it/s]\n",
      "Running evaluation: 7it [00:00, 101.55it/s]\n",
      "Running evaluation: 7it [00:00, 102.17it/s]\n",
      "Running evaluation: 7it [00:00, 100.76it/s]\n",
      "Running evaluation: 7it [00:00, 102.63it/s]\n",
      "Running evaluation: 7it [00:00, 103.41it/s]\n",
      "Running evaluation: 7it [00:00, 103.52it/s]\n",
      "Running evaluation: 7it [00:00, 103.16it/s]\n",
      "Running evaluation: 7it [00:00, 103.82it/s]\n",
      "Running evaluation: 7it [00:00, 104.20it/s]\n",
      "Running evaluation: 7it [00:00, 103.68it/s]\n",
      "Running evaluation: 7it [00:00, 104.24it/s]\n",
      "Running evaluation: 7it [00:00, 103.54it/s]\n",
      "Running evaluation: 7it [00:00, 103.79it/s]\n",
      "Running evaluation: 7it [00:00, 103.73it/s]\n",
      "Running evaluation: 7it [00:00, 103.74it/s]\n",
      "Running evaluation: 7it [00:00, 103.96it/s]\n",
      "Running evaluation: 7it [00:00, 103.54it/s]\n",
      "Running evaluation: 7it [00:00, 103.85it/s]\n",
      "Running evaluation: 7it [00:00, 103.44it/s]\n",
      "Running evaluation: 7it [00:00, 103.30it/s]\n",
      "Running evaluation: 7it [00:00, 103.44it/s]\n",
      "Running evaluation: 7it [00:00, 103.34it/s]\n",
      "Running evaluation: 7it [00:00, 103.68it/s]\n",
      "Running evaluation: 7it [00:00, 103.80it/s]\n",
      "Running evaluation: 7it [00:00, 103.45it/s]\n",
      "Running evaluation: 7it [00:00, 103.46it/s]\n",
      "Running evaluation: 7it [00:00, 101.78it/s]\n",
      "Running evaluation: 7it [00:00, 103.25it/s]\n",
      "Running evaluation: 7it [00:00, 103.12it/s]\n",
      "Running evaluation: 7it [00:00, 102.66it/s]\n",
      "Running evaluation: 7it [00:00, 102.86it/s]\n",
      "Running evaluation: 7it [00:00, 101.91it/s]\n",
      "Running evaluation: 7it [00:00, 102.14it/s]\n",
      "Running evaluation: 7it [00:00, 103.50it/s]\n",
      "Running evaluation: 7it [00:00, 102.48it/s]\n",
      "Running evaluation: 7it [00:00, 102.02it/s]\n",
      "Running evaluation: 7it [00:00, 103.05it/s]\n",
      "Running evaluation: 7it [00:00, 101.83it/s]\n",
      "Running evaluation: 7it [00:00, 101.95it/s]\n",
      "Running evaluation: 7it [00:00, 103.24it/s]\n",
      "Running evaluation: 7it [00:00, 84.71it/s]\n"
     ]
    }
   ],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(dataset=dataset_test,\n",
    "                                             predictor=predictor,\n",
    "                                             num_samples=100)\n",
    "forecasts = list(forecast_it)\n",
    "targets = list(ts_it)\n",
    "agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS: 0.48857047034606677\n",
      "ND: 0.5834631932625387\n",
      "NRMSE: 1.1737168856721258\n",
      "MSE: 1306.7860587706446\n"
     ]
    }
   ],
   "source": [
    "print(\"CRPS: {}\".format(agg_metric['mean_wQuantileLoss']))\n",
    "print(\"ND: {}\".format(agg_metric['ND']))\n",
    "print(\"NRMSE: {}\".format(agg_metric['NRMSE']))\n",
    "print(\"MSE: {}\".format(agg_metric['MSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS-Sum: 0.4911344613270915\n",
      "ND-Sum: 0.5287813955218926\n",
      "NRMSE-Sum: 0.9606235665322241\n",
      "MSE-Sum: 16429540.476190476\n"
     ]
    }
   ],
   "source": [
    "print(\"CRPS-Sum: {}\".format(agg_metric['m_sum_mean_wQuantileLoss']))\n",
    "print(\"ND-Sum: {}\".format(agg_metric['m_sum_ND']))\n",
    "print(\"NRMSE-Sum: {}\".format(agg_metric['m_sum_NRMSE']))\n",
    "print(\"MSE-Sum: {}\".format(agg_metric['m_sum_MSE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
