{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "\n",
    "from pts.dataset import recipe as rcp\n",
    "\n",
    "\n",
    "TIME_SERIE_LENGTH = 10\n",
    "PREDICTION_LENGTH = 5\n",
    "NUMBER_OF_TIME_SERIES = 1000\n",
    "        \n",
    "def write_to_file(recipe, length, num_ts, file_name):\n",
    "    with open(\"{}.json\".format(file_name), 'w') as f:\n",
    "        for x in islice(rcp.generate(length, recipe, \"2019-01-07 00:00\"), num_ts):\n",
    "            z = {}\n",
    "            for k in x:\n",
    "                if type(x[k]) == np.ndarray:\n",
    "                    z[k] = x[k].tolist()\n",
    "                else:\n",
    "                    z[k] = x[k]\n",
    "            f.write(json.dumps(z))\n",
    "            f.write('\\n')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "DATASET_REFERENCES = {\n",
    "    \"gaussian\": 4 + rcp.RandomGaussian()\n",
    "}        \n",
    "\n",
    "for fn in DATASET_REFERENCES:\n",
    "    recipe = [(\"input\", 1.), (\"target\", DATASET_REFERENCES[fn])]\n",
    "    \n",
    "    write_to_file(recipe, length=TIME_SERIE_LENGTH, num_ts=NUMBER_OF_TIME_SERIES, file_name=fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117it [00:02, 40.59it/s, avg_epoch_loss=0.814, epoch=0]"
     ]
    }
   ],
   "source": [
    "from pts.dataset.artificial import RecipeDataset\n",
    "from pts.dataset import FileDataset, MetaData\n",
    "\n",
    "from pts.dataset.repository import get_dataset\n",
    "from pts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from pts.model.deepar import DeepAREstimator\n",
    "from pts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from pts.modules import (\n",
    "    ImplicitQuantileOutput,\n",
    "    PiecewiseLinearOutput,\n",
    ")\n",
    "from pts import Trainer\n",
    "\n",
    "metadata = MetaData(freq=\"H\", prediction_length=PREDICTION_LENGTH)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "NUM_BATCHES_PER_EPOCH = 120\n",
    "NUM_TRAININGS = 5\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "def run_one_training(dataset, distr_output):\n",
    "    estimator = SimpleFeedForwardEstimator(\n",
    "        distr_output=distr_output,\n",
    "        freq=metadata.freq,\n",
    "        prediction_length=metadata.prediction_length,\n",
    "        num_hidden_dimensions=[10],\n",
    "        trainer=Trainer(device=\"cpu\",\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        learning_rate=1e-3,\n",
    "                        num_batches_per_epoch=NUM_BATCHES_PER_EPOCH,\n",
    "                        batch_size=256,\n",
    "                        num_workers=1,\n",
    "                        ),\n",
    "    )\n",
    "    predictor = estimator.train(dataset)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=NUM_SAMPLES,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    evaluator = Evaluator()\n",
    "    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(dataset))\n",
    "    \n",
    "    all_quantiles_df = pd.DataFrame(\n",
    "        [\n",
    "            [forecasts[item_id].quantile(q/100.)[0] for q in range(0, 100)] \n",
    "            for item_id in range(len(forecasts))\n",
    "        ]\n",
    "    )\n",
    "    sampled_quantiles = all_quantiles_df.mean(axis=0)\n",
    "    return estimator, agg_metrics, sampled_quantiles\n",
    "\n",
    "\n",
    "def run_several_training(dataset, distr_output, num_trainings=NUM_TRAININGS):\n",
    "    estimators = []\n",
    "    all_agg_metrics = []\n",
    "    all_sampled_quantiles = []\n",
    "    for i in range(num_trainings):\n",
    "        estimator, agg_metrics, sampled_quantiles = run_one_training(dataset, distr_output)\n",
    "        estimators.append(estimator)\n",
    "        all_agg_metrics.append(agg_metrics)\n",
    "        all_sampled_quantiles.append(sampled_quantiles)\n",
    "        \n",
    "    return estimators, all_agg_metrics, all_sampled_quantiles\n",
    "\n",
    "def plot_quantile_functions(all_sampled_quantiles):\n",
    "    for serie in all_sampled_quantiles:\n",
    "        plt.plot(serie)\n",
    "\n",
    "def summarize_metrics(all_agg_metrics):\n",
    "    KEY_METRICS = [\"MASE\", \"sMAPE\", \"MSIS\"]\n",
    "    agg_func = [np.mean, np.min, np.max]\n",
    "\n",
    "    all_metrics = pd.DataFrame(all_agg_metrics)\n",
    "    QUANTILE_METRICS = [c for c in all_metrics.columns if (\"Coverage\" in c) or (\"wQuantile\" in c)]\n",
    "    \n",
    "    display(all_metrics[KEY_METRICS].agg(agg_func, axis=0).T)\n",
    "    display(all_metrics[QUANTILE_METRICS].agg(agg_func, axis=0).T)\n",
    "    \n",
    "\n",
    "store_results = {}\n",
    "distr_outputs = {\n",
    "    'iqn': ImplicitQuantileOutput(output_domain=\"Real\"),\n",
    "    'piecewiseLinear': PiecewiseLinearOutput(num_pieces=15)\n",
    "}\n",
    "\n",
    "\n",
    "for fn in DATASET_REFERENCES:\n",
    "    dataset = FileDataset(\"{}.json\".format(fn), metadata.freq, shuffle=True)\n",
    "    for dn in distr_outputs:\n",
    "        distr_output = distr_outputs[dn]\n",
    "        estimators, all_agg_metrics, all_sampled_quantiles = run_several_training(dataset, distr_output)\n",
    "        store_results[(fn, dn)] = {\n",
    "            \"estimators\": estimators,\n",
    "            \"all_agg_metrics\": all_agg_metrics, \n",
    "            \"all_sampled_quantiles\": all_sampled_quantiles,\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in DATASET_REFERENCES:\n",
    "    true_distribution = DATASET_REFERENCES[fn]\n",
    "    random_values = rcp.evaluate([('target', test)], length=10000)['target']\n",
    "    true_quantiles = [np.quantile(output, q=q/100) for q in range(100)]\n",
    "    \n",
    "    for dn in distr_outputs:\n",
    "        all_sampled_quantiles = store_results[(fn, dn)][\"all_sampled_quantiles\"]\n",
    "        plt.plot(true_quantiles, color=\"black\", linestyle='dashed',)\n",
    "        plot_quantile_functions(all_sampled_quantiles)\n",
    "        plt.legend([\"True distribution\"])\n",
    "        plt.title(\"Distribution: {}, Estimator: {}\".format(fn, dn))\n",
    "        plt.show()\n",
    "    \n",
    "    for dn in distr_outputs:\n",
    "        all_agg_metrics = store_results[(fn, dn)][\"all_agg_metrics\"]\n",
    "        summarize_metrics(all_agg_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
